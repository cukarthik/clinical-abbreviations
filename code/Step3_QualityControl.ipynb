{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Quality Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from spellchecker import SpellChecker\n",
    "from master_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress false positive warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/lisavirginia/clinical-abbreviations/master/code/Step2Output.csv',\n",
    "                 sep='|',\n",
    "                 header=0,\n",
    "                 index_col=False,\n",
    "                 na_filter=False,\n",
    "                 dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heuristic 1: Duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify which records exactly duplicate another record from the same source. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Extract1 = df[df.duplicated(['SF', 'LF', 'Source']) == True]\n",
    "Extract1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heuristic 2: Punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify excess punctuation in the long form (e.g. \"nitric oxide;\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Punctuation after LF (excludes .+%()[])\n",
    "Extract2_1 = df[df['LF'].str.contains('.*[,\\/#!\\$\\^&@\\?<>\\*:;{}=\\-_\\'~\\\"]$') == True]\n",
    "Extract2_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Punctuation before LF (excludes .+%()[])\n",
    "Extract2_2 = df[df['LF'].str.contains('^[,\\/#!\\$\\^&@\\?<>\\*:;{}=\\-_\\'~\\\"].*') == True]\n",
    "Extract2_2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify excess punctuation in the short form (e.g. \"..IVF\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excess periods before SF\n",
    "Extract2_3 = df[df['SF'].str.contains('^[\\.]+.*') == True]\n",
    "Extract2_3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heuristic 3: Spelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The long form contains spelling errors (e.g. \"cncer\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set spell checker parameters\n",
    "spell = SpellChecker(distance=1)\n",
    "\n",
    "# Add medical word corpus (UMLS Metathesaurus)\n",
    "spell.word_frequency.load_text_file('data/ClinSpell.txt')\n",
    "\n",
    "# Exclude UMLS and ADAM\n",
    "subset = df[(df['Source'] != 'UMLS') & \n",
    "            (df['Source'] != 'ADAM')]\n",
    "\n",
    "# Instantiate output\n",
    "misspelled_rows = []\n",
    "misspelled_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over subset\n",
    "for index, row in subset.iterrows():\n",
    "    \n",
    "    # Format LF for spellchecker\n",
    "    pre_token = re.sub('[^A-Za-z\\s\\-]+', '', row['LF']).lower()\n",
    "    token = list(filter(None, re.split(r'[\\s\\-]+', pre_token)))\n",
    "    \n",
    "    # Identify misspelled LFs\n",
    "    misspelled = spell.unknown(token)\n",
    "    if len(misspelled) > 0:\n",
    "        misspelled_rows.append(row['RecordID'])\n",
    "        misspelled_data.append(misspelled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract misspelled LFs\n",
    "Extract3 = df[df['RecordID'].isin(misspelled_rows)]\n",
    "Extract3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heuristic 4: Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The alphabetic characters in the short form don't occur anywhere in the long form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include problematic sources\n",
    "subset = df[(df['Source'] == 'Vanderbilt Clinic Notes') | \n",
    "            (df['Source'] == 'Vanderbilt Discharge Sums')]\n",
    "\n",
    "# Instantiate output\n",
    "missing_character = []\n",
    "missing_char_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over dataframe\n",
    "for index, row in subset.iterrows():\n",
    "    \n",
    "    # Extract alphabetic characters\n",
    "    alph_SF = set(re.sub('[^A-Za-z]+', '', row['SF']).lower())\n",
    "    alph_LF = set(re.sub('[^A-Za-z]+', '', row['LF']).lower())\n",
    "    \n",
    "    if alph_SF.issubset(alph_LF) == False:\n",
    "        if (alph_SF - alph_LF) != {'x'}:\n",
    "            missing_character.append(row['RecordID'])\n",
    "            missing_char_data.append(alph_SF - alph_LF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract LFs missing characters\n",
    "Extract4 = df[df['RecordID'].isin(missing_character)]\n",
    "Extract4.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heuristic 5: User-Identified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Extract5 = df[(df['LF'].str.contains(\"#000066\") |\n",
    "              df['LF'].str.contains(\"typo\") |\n",
    "              df['LF'].str.contains(\"not an abbreviation\") | \n",
    "              df['LF'].str.contains(\"not an acronym\"))]\n",
    "Extract5.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error type, decision, modification\n",
    "Extract1['error'], Extract1['action'] = [\"duplicate\", \"retire\"]\n",
    "Extract2_1['error'], Extract2_1['action'] = [\"punctuation after LF\", \"modify\"]\n",
    "Extract2_2['error'], Extract2_2['action'] = [\"punctuation before LF\", \"modify\"]\n",
    "Extract2_3['error'], Extract2_3['action'] = [\"punctuation before SF\", \"modify\"]\n",
    "Extract3['error'], Extract3['action'] = [misspelled_data, \"modify\"]\n",
    "Extract4['error'], Extract4['action'] = [missing_char_data, \"modify\"]\n",
    "Extract5['error'], Extract5['action'] = [\"user identified\", \"retire\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = pd.concat([Extract1, Extract2_1, Extract2_2, Extract2_3, Extract3, Extract4, Extract5])\n",
    "errors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = errors.drop_duplicates(subset=\"RecordID\")\n",
    "errors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors.to_csv('data/Errors_Automated.csv',\n",
    "              index=False,\n",
    "              header=True,\n",
    "              sep='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Annotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = pd.read_csv('data/Errors_Annotated.csv',\n",
    "                     sep='|',\n",
    "                     header=0,\n",
    "                     index_col=False,\n",
    "                     na_filter=False,\n",
    "                     dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors.sample(3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors['action'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = errors[(errors['action'] != 'none')]\n",
    "errors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subset Crosswalk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = df # Keep unsubsetted version\n",
    "df = df[~df['RecordID'].isin(errors['RecordID'])]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subset Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retire = df_all[df_all['RecordID'].isin(errors[(errors['action'] == 'retire')]['RecordID'])]\n",
    "retire.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modify = errors[(errors['action'] == 'modify')].iloc[:, 0:19]\n",
    "modify.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retire Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify duplicates\n",
    "dups = pd.concat([df, modify])\n",
    "dups = dups[dups.duplicated(['SF', 'LF', 'Source']) == True]\n",
    "dups.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove from modify\n",
    "modify = modify[~modify['RecordID'].isin(dups['RecordID'])]\n",
    "modify = modify.reset_index(drop=True)\n",
    "modify.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to retire\n",
    "retire = pd.concat([retire, df_all[df_all['RecordID'].isin(dups['RecordID'])]])\n",
    "retire = retire.reset_index(drop=True)\n",
    "retire.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Strip Source Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is done as the source data is potentially no longer valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modify['SFUI'], modify['NormSF'], modify['NSFUI'], modify['PrefSF'] = ['', '', '', '']\n",
    "modify['LFUI'], modify['NormLF'], modify['PrefLF'], modify['SFEUI'] = ['', '', '', '']\n",
    "modify['LFEUI'], modify['Type'], modify['Score'], modify['Count'] = ['', '', '', '']\n",
    "modify['Frequency'], modify['UMLS.CUI'] = ['', '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modify.sample(3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reassign Normalized Short Form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modify['NormSF'] = modify['SF'].apply(normalized_short_form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modify.sample(3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reassign SFUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search existing SFUIs\n",
    "for index, row in modify.iterrows():\n",
    "    temp = df_all[(df_all['SF'] == modify['SF'].iat[index])]\n",
    "    if temp.empty:\n",
    "        modify['SFUI'].iat[index] = ''\n",
    "    else:\n",
    "        modify['SFUI'].iat[index] = temp.iloc[0]['SFUI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If none, add SFUI\n",
    "modify = add_new_SFUI(modify)\n",
    "modify.sample(3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reassign LFUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search existing LFUIs\n",
    "for index, row in modify.iterrows():\n",
    "    temp = df_all[(df_all['LF'] == modify['LF'].iat[index])]\n",
    "    if temp.empty:\n",
    "        modify['LFUI'].iat[index] = ''\n",
    "    else:\n",
    "        modify['LFUI'].iat[index] = temp.iloc[0]['LFUI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If none, add LFUI\n",
    "modify = add_new_LFUI(modify)\n",
    "modify.sample(3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add \"Modified\" Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modify[\"Modified\"] = \"Modified\"\n",
    "df[\"Modified\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Append to Crosswalk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, modify])\n",
    "df = df.sort_values(by=['RecordID'])\n",
    "df = df.reset_index(drop=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export Modify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get original rows\n",
    "modify = df_all[df_all['RecordID'].isin(modify['RecordID'])]\n",
    "modify.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modify.to_csv('ModifiedRecords.csv',\n",
    "              index=False,\n",
    "              header=True,\n",
    "              sep='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export Retire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retire.to_csv('RetiredRecords.csv',\n",
    "              index=False,\n",
    "              header=True,\n",
    "              sep='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export Crosswalk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Step3Output.csv',\n",
    "          index=False,\n",
    "          header=True,\n",
    "          sep='|')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (venv_py37_full)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
